{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS429: Information Retrieval\n",
    "\n",
    "<br>\n",
    "\n",
    "## Lecture 16: Naive Bayes\n",
    "\n",
    "<br>\n",
    "\n",
    "### Dr. Aron Culotta\n",
    "### Illinois Institute of Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall classification problem notation:\n",
    "\n",
    "\n",
    "- $\\vec{x} \\in \\mathcal{X}$ &nbsp;&nbsp;&nbsp;&nbsp; *instance*, *example*, *input*\n",
    "  - e.g., an email\n",
    "- $y \\in \\mathcal{Y}$ &nbsp;&nbsp;&nbsp;&nbsp; *target*, *class*, *label*, *output*\n",
    "  - e.g., $y=1$: spam ; $y=-1$: not spam\n",
    "- $f: \\mathcal{X} \\mapsto \\mathcal{Y}$ &nbsp;&nbsp;&nbsp;&nbsp; *hypothesis*, *learner*, *model*, *classifier*\n",
    "  - e.g., if $x$ contain the word *free*, $y$ is $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training data:**\n",
    "\n",
    "We are given training data $D = \\{(\\vec{x}_1, y_1), \\ldots, (\\vec{x}_n, y_n)\\}$\n",
    "\n",
    "||free|money| |*label*|\n",
    "|:--:|:--------:|:--------:|:--:|:--:|\n",
    "||$x_{i1}$|$x_{i2}$| | $y_i$ |\n",
    "|$x_1$|0|0||-1| \n",
    "|$x_2$|1|0|| 1|\n",
    "|$x_3$|1|1||-1|\n",
    "|$x_4$|1|0||-1|\n",
    "|$x_5$|1|1||1|\n",
    "|$x_6$|0|0||1|\n",
    "|$x_7$|0|1||-1|\n",
    "\n",
    "How to classify a new instance?  \n",
    " \"free money\" -> $\\{1,1\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall Bayes' Rule:**\n",
    "\n",
    "$$\n",
    "p(y|x) = \\frac{p(x|y)p(y)}{p(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify an instance $\\vec{x}$, we need $p(y|\\vec{x})$.\n",
    "\n",
    "E.g., if $p(y=1|\\vec{x}) > p(y=-1|\\vec{x})$, then classify $\\vec{x}$ as 1.\n",
    "\n",
    "Using Bayes' rule, we can rewrite $p(y|\\vec{x})$ as:\n",
    "\n",
    "$$\n",
    "p(y|\\vec{x}) = \\frac{p(\\vec{x}|y)p(y)}{p(\\vec{x})}\n",
    "$$\n",
    "\n",
    "<br><br><br>\n",
    "Three terms:\n",
    "\n",
    "- $p(y)$: **prior** probability of class y.\n",
    "$$p(y=1) = \\frac{\\sum_{(x_i, y_i) \\in D} 1[y_i=1]}{|D|}$$\n",
    " - $1[x]= 1 $ if $x$ is True, $0$ otherwise.\n",
    "<br><br><br>\n",
    "\n",
    "- $p(\\vec{x})$: **evidence** (probability of this document)\n",
    "$$p(\\vec{x}) = \\sum_{y^k \\in \\{-1, 1\\}} p(\\vec{x} | y=y^k)p(y=y^k)$$\n",
    "  - This is just the sum of the numerator for all settings of $y$\n",
    "  \n",
    "<br><br><br>\n",
    "\n",
    "- $p(\\vec{x}|y)$ **likelihood**\n",
    "  - This is harder to estimate.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "**Estimating $p(\\vec{x}|y)$:**\n",
    "\n",
    "<table width=40%>\n",
    "<tr> <th width=10%> free</th> <th width=10%>money</th> <th width=40%>$p(\\vec{x}|y=1)$</th> <th width=40%>$p(\\vec{x}|y=-1)$</th> </tr>\n",
    "<tr><td>0</td><td>0</td><td>?</td><td>?</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>?</td><td>?</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>?</td><td>?</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>?</td><td>?</td></tr>\n",
    "</table>\n",
    "(assuming binary feature values)\n",
    "\n",
    "\n",
    "We'd like to be able to compute this just like $p(y)$:\n",
    "\n",
    "$$\n",
    "p(\\vec{x}=\\{0,0\\}|y=1) = \\frac{\\sum_{(x_i, y_i) \\in D} 1[\\vec{x}_i=\\{0,0\\}]}{\\sum_i 1[y_i=1]}\n",
    "$$\n",
    "i.e., what percentage of documents with label $y=1$ have feature vector $\\{0, 0\\}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Clearly, this does not scale. \n",
    "\n",
    "Size of table above?\n",
    "<br><br><br>\n",
    "\n",
    "$k*2^d$, where $d$ is the number of features and $k$ is the number of classes. In text classification, $d$ is often in the tens of thousands.\n",
    "\n",
    "Even more ridiculous if we move from binary features to feature counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we make a conditional independence assumption (just like in the [Binary Independence Model](https://github.com/iit-cs429/main/tree/master/lectures/lec11)).\n",
    "\n",
    "This assumption is what is Naïve about Naïve Bayes.\n",
    "\n",
    "Racell, conditional independence means that $p(a,b|c) = p(a|c)p(b|c)$. Here, we assume each feature value is independent of others given the class label: \n",
    "\n",
    "$$p(\\vec{x_i} | y=1) =  p(\\{x_{i1}, x_{i2}, \\ldots x_{id}) \\approx  p(\\{x_{i1}|y=1) p(x_{i2}|y=1) \\ldots p(x_{id}|y=1)$$\n",
    "\n",
    "Thus, our table of parameters to estimate becomes linear:\n",
    "\n",
    "<table width=40%>\n",
    "<tr> <th width=10%> free</th>  <th width=40%>$p(free|y=1)$</th> <th width=40%>$p(free|y=-1)$</th> </tr>\n",
    "<tr><td>0</td><td>?</td><td>?</td></tr>\n",
    "<tr><td>1</td><td>?</td><td>?</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table width=40%>\n",
    "<tr> <th width=10%> money</th>  <th width=40%>$p(money|y=1)$</th> <th width=40%>$p(money|y=-1)$</th> </tr>\n",
    "<tr><td>0</td><td>?</td><td>?</td></tr>\n",
    "<tr><td>1</td><td>?</td><td>?</td></tr>\n",
    "</table>\n",
    "\n",
    "Number of parameters: $k*d$, where $d$ is the number of features and $k$ is the number of classes.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Our classification formula then becomes:\n",
    "\n",
    "$$\n",
    "p(y|\\vec{x}) = \\frac{p(y)\\prod_j p(x_{ij}|y)}{p(\\vec{x})}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimating $p(x_{ij}|y)$:**\n",
    "\n",
    "When term features are **binary**, we call this a **[Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution)** model (word presence is outcome of a biased coin flip)\n",
    "\n",
    "$$p(x_{k}=1|y=1) = \\frac{\\sum_{(x_i, y_i) \\in D}1[x_{ik}=1 \\wedge y_i=1]}{\\sum_{(x_i, y_i) \\in D} 1[y_i=1]}$$\n",
    "i.e., what proportion of documents where $y=1$ have term $k$?\n",
    "\n",
    "<br><br>\n",
    "\n",
    "We can compute similar values for term absence, as well as for other classes:\n",
    "\n",
    "$$p(x_{k}=0|y=1) = \\frac{\\sum_{(x_i, y_i) \\in D}1[x_{ik}=0 \\wedge y_i=1]}{\\sum_{(x_i, y_i) \\in D} 1[y_i=1]}$$\n",
    "i.e., what proportion of documents where $y=1$ **do not** have term $k$?\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$p(x_{k}=1|y=-1) = \\frac{\\sum_{(x_i, y_i) \\in D}1[x_{ik}=1 \\wedge y_i=-1]}{\\sum_{(x_i, y_i) \\in D} 1[y_i=-1]}$$\n",
    "i.e., what proportion of documents where $y=-1$ have term $k$?\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$p(x_{k}=0|y=-1) = \\frac{\\sum_{(x_i, y_i) \\in D}1[x_{ik}=0 \\wedge y_i=-1]}{\\sum_{(x_i, y_i) \\in D} 1[y_i=-1]}$$\n",
    "i.e., what proportion of documents where $y=-1$ **do not** have term $k$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a specific set of training data:\n",
    "\n",
    "||free|money| |*label*|\n",
    "|:--:|:--------:|:--------:|:--:|:--:|\n",
    "||$x_{i1}$|$x_{i2}$| | $y_i$ |\n",
    "|$x_1$|0|0||-1| \n",
    "|$x_2$|1|0|| 1|\n",
    "|$x_3$|1|1||-1|\n",
    "|$x_4$|1|0||-1|\n",
    "|$x_5$|1|1||1|\n",
    "|$x_6$|0|0||1|\n",
    "|$x_7$|0|1||-1|\n",
    "\n",
    "\n",
    "- $p(free=1|y=1) = \\frac{2}{3}$\n",
    "- $p(free=0|y=1) = \\frac{1}{3}$\n",
    "- $p(free=1|y=-1) = \\frac{2}{4}$\n",
    "- $p(free=0|y=-1) = \\frac{2}{4}$\n",
    "<br><br>\n",
    "- $p(money=1|y=1) = \\frac{1}{3}$\n",
    "- $p(money=0|y=1) = \\frac{2}{3}$\n",
    "- $p(money=1|y=-1) = \\frac{2}{4}$\n",
    "- $p(money=0|y=-1) = \\frac{2}{4}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that $p(x=0|y=1) = 1 - p(x=1|y=1)$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Estimate Prior:**\n",
    "\n",
    "- $p(y=1) = \\frac{3}{7}$\n",
    "- $p(y=-1) = \\frac{4}{7}$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Compute probability for a new document:**\n",
    "\n",
    "$\\vec{x} = $ \"free money\" = $\\{1, 1\\}$\n",
    "\n",
    "$$\n",
    "p(y=1|\\vec{x})  =  \\frac{p(y=1)\\prod_j p(x_{ij}|y=1)}{p(\\vec{x})} = \\frac{p(y=1)p(free=1|y=1)p(money=1|y=1)}{p(\\vec{x})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\\frac{3}{7} \\cdot \\frac{2}{3} \\cdot \\frac{1}{3}}{p(\\vec{x})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(y=-1|\\vec{x}) = \\frac{\\frac{4}{7} \\cdot \\frac{2}{4} \\cdot \\frac{2}{4}}{p(\\vec{x})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\vec{x}) = (\\frac{3}{7} \\cdot \\frac{2}{3} \\cdot \\frac{1}{3}) + (\\frac{4}{7} \\cdot \\frac{2}{4} \\cdot \\frac{2}{4}) = .238...\n",
    "$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\n",
    "p(y=1|\\vec{x}) =  \\frac{\\frac{3}{7} \\cdot \\frac{2}{3} \\cdot \\frac{1}{3}}{.238...} =  .4\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(y=-1|\\vec{x}) =  \\frac{\\frac{4}{7} \\cdot \\frac{2}{4} \\cdot \\frac{2}{4}}{.238...} = .6\n",
    "$$\n",
    "\n",
    "Note that $p(y=1|\\vec{x}) + p(y=-1|\\vec{x}) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laplacian smoothing: **\n",
    "\n",
    "To avoid 0-probabilities in the calculations above, we can simply add a small value to each count, and likewise update the normalizer so terms sum to 1. For\n",
    "\n",
    "$$p(x_{k}=1|y=1) = \\frac{\\sum_{(x_i, y_i) \\in D}1[x_{ik}=1 \\wedge y_i=1] + \\epsilon}{2 \\epsilon + \\sum_{(x_i, y_i) \\in D} 1[y_i=1]}$$\n",
    "\n",
    "Commonly, $\\epsilon=1$ is used (“plus one” smoothing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Event Model**\n",
    "\n",
    "The preceeding assumes a binary event model; that is, $x_{ij} \\in \\{0,1\\}$. Alternatively, we can use term frequencies; i.e. $x_{ij} \\in \\mathcal{N}_+$. The term probabilities become:\n",
    "\n",
    "$$\n",
    "p(x_{ij}=1 | y_i = 1) = \\frac{T_{1j}}{\\sum_k T_{1k}}\n",
    "$$\n",
    "\n",
    "where $T_{cj}$ is the number occurrences of term $j$ in documents where $y=c$. E.g., count all the occurrences of the term $j$ in documents where the true class label is $c$. You can use the analogous equation for class $-1$, $p(x_{ij}=1 | y_i=-1)$.\n",
    "\n",
    "Smoothing operates differently for Multinomial than Bernoulli Naive Bayes:\n",
    "\n",
    "$$\n",
    "p(x_{ij}=1 | y_i = 1) = \\frac{T_{1j} + \\epsilon}{|V|\\epsilon + \\sum_k T_{1k}}\n",
    "$$\n",
    "\n",
    "where $|V|$ is the number of unique terms in the vocabulary.\n",
    "\n",
    "Note that in Multinomial Naive Bayes, to classify a new document, we only multiply terms that occur in the document: \n",
    "\n",
    "$$\n",
    "p(y_i|x_i) = \\frac{ p(y_i) \\prod_{j \\in n_i} p(x_{ij}|y_i)} {p(\\vec{x})}\n",
    "$$\n",
    "\n",
    "where $n_i$ iterates over <span>**tokens**</span>, rather then <span>**terms**</span>. That is, if the term <span>*dog*</span> occurs twice in document $i$, its corresponding probability $p(dog|y_i)$ will appear twice in the product above.\n",
    "\n",
    "See your book ([Ch13](http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf)) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mnnb](images/mnnb.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
