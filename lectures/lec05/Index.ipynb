{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 429: Information Retrieval\n",
    "\n",
    "<br>\n",
    "\n",
    "## Lecture 5: Scalable Indexing\n",
    "\n",
    "<br>\n",
    "\n",
    "### Dr. Aron Culotta\n",
    "### Illinois Institute of Technology \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Last time:\n",
    "\n",
    "- Efficient retrieval of postings lists\n",
    "- Wildcard queries\n",
    "- Spelling correction\n",
    "\n",
    "Today: \n",
    "\n",
    "- How do we build an index that does not fit into memory? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building an index\n",
    "\n",
    "- Up to now, we've assumed everything fits in memory.\n",
    "- We'll discuss three ways to scale\n",
    "  1. Block sort-based indexing (**BSBI**)\n",
    "  2. Single-pass in-memory indexing (**SPIMI**)\n",
    "  3. MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How long does it take to read 100MB from disk?\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Seek time** (to locate data)\n",
    "- **Transfer rate** (to copy from disk into memory)\n",
    "- Contiguous or non-contiguous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Block sort-based indexing (BSBI)\n",
    "\n",
    "Assume a single machine.\n",
    "\n",
    "1. Split documents into **blocks**\n",
    "2. For each block:\n",
    "  1. Parse each block into (word_id, doc_id) pairs\n",
    "  2. Sort pairs and create separate postings lists for each block.\n",
    "  3. Write postings lists to disk\n",
    "3. Merge the postings lists file for each block\n",
    "\n",
    "![bsbi](files/bsbi.png)\n",
    "\n",
    "(source: [MRS](http://nlp.stanford.edu/IR-book/pdf/04const.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "block 0 postings= [(0, 0), (1, 0), (2, 0), (0, 1), (3, 1), (2, 1)]\n",
      "\n",
      "vocab= dict_items([('dog', 1), ('cat', 3), ('jumped', 2), ('the', 0)])\n",
      "\n",
      "block 0 sorted postings= [(0, 0), (0, 1), (1, 0), (2, 0), (2, 1), (3, 1)]\n",
      "\n",
      "block 0 grouped postings= [(0, [0, 1]), (1, [0]), (2, [0, 1]), (3, [1])]\n",
      "\n",
      "\n",
      "block 1 postings= [(4, 2), (1, 2), (5, 2), (0, 3), (6, 3), (2, 3)]\n",
      "\n",
      "vocab= dict_items([('a', 4), ('ran', 5), ('the', 0), ('zebra', 6), ('dog', 1), ('cat', 3), ('jumped', 2)])\n",
      "\n",
      "block 1 sorted postings= [(0, 3), (1, 2), (2, 3), (4, 2), (5, 2), (6, 3)]\n",
      "\n",
      "block 1 grouped postings= [(0, [3]), (1, [2]), (2, [3]), (4, [2]), (5, [2]), (6, [3])]\n"
     ]
    }
   ],
   "source": [
    "# BSBI: Create one postings list per block of documents.\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "\n",
    "block1 = [(0, [\"the\", \"dog\", \"jumped\"]),\n",
    "          (1, [\"the\", \"cat\", \"jumped\"])]\n",
    "\n",
    "block2 = [(2, [\"a\", \"dog\", \"ran\"]),\n",
    "          (3, [\"the\", \"zebra\", \"jumped\"])]\n",
    "\n",
    "blocks = [block1, block2]\n",
    "\n",
    "vocab = defaultdict(lambda: len(vocab))  # auto-increment word indices\n",
    "\n",
    "for block_id, block in enumerate(blocks):\n",
    "    # A. Collect all individual postings: (word_id, doc_id) pairs.\n",
    "    postings = []\n",
    "    for doc_id, doc in block:\n",
    "        for word in doc:\n",
    "            postings.append((vocab[word], doc_id))\n",
    "    print('\\n\\nblock', block_id, 'postings=', postings)\n",
    "    print('\\nvocab=', vocab.items())\n",
    "\n",
    "    # B. Sort postings and create postings lists.\n",
    "    postings = sorted(postings)  # sorts by first element of tuple by default\n",
    "    print('\\nblock', block_id, 'sorted postings=', postings)\n",
    "    \n",
    "    # Group postings for same term together.\n",
    "    postings = groupby(postings, key=lambda x:x[0])\n",
    "    postings = [(word_id, [g[1] for g in group]) for word_id, group in postings]\n",
    "    print('\\nblock', block_id, 'grouped postings=', postings)\n",
    "    \n",
    "    # C. Write to disk\n",
    "    f = open('bsbi_block' + str(block_id) + '.txt', 'wt')\n",
    "    f.write('\\n'.join(['%s' % str(p) for p in postings]))\n",
    "    f.close()\n",
    "    print\n",
    "    \n",
    "# Then, merge blocks in linear time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Merging posting list blocks\n",
    "\n",
    "Since each block is sorted by term, can do a single linear pass through each block.\n",
    "\n",
    "- Open all postings files simulataneously.\n",
    "- Read small buffer from each.\n",
    "- Equivalent to a union of postings lists for same term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BSBI\n",
    "\n",
    "Space requirements?\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Number of tokens in each block ($T$)\n",
    "- Number of unique terms in vocabulary ($V$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Time requirements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sorting (word_id, doc_id) pairs in each block.\n",
    "- $O(T log T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single-pass in-memory indexing (SPIMI)\n",
    "\n",
    "- separate dictionary for each block\n",
    "- create postings lists on the fly, rather than collecting all postings then sorting.\n",
    "- sort postings lists, rather than individual postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Block 0 ['cat [1]', 'dog [0]', 'jumped [0, 1]', 'the [0, 1]']\n",
      "\n",
      "\t dict_items([('dog', 1), ('cat', 3), ('jumped', 2), ('the', 0)])\n",
      "\n",
      "\n",
      "Block 1 ['a [2]', 'dog [2]', 'jumped [3]', 'ran [2]', 'the [3]', 'zebra [3]']\n",
      "\n",
      "\t dict_items([('a', 0), ('ran', 2), ('the', 3), ('zebra', 4), ('dog', 1), ('jumped', 5)])\n"
     ]
    }
   ],
   "source": [
    "# SPIMI: Create one postings list per block of documents.\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "\n",
    "block1 = [(0, [\"the\", \"dog\", \"jumped\"]),\n",
    "          (1, [\"the\", \"cat\", \"jumped\"])]\n",
    "\n",
    "block2 = [(2, [\"a\", \"dog\", \"ran\"]),\n",
    "          (3, [\"the\", \"zebra\", \"jumped\"])]\n",
    "\n",
    "blocks = [block1, block2]\n",
    "\n",
    "for block_id, block in enumerate(blocks):\n",
    "    # Note that there is a new vocab for each block!\n",
    "    vocab = defaultdict(lambda: len(vocab))  # maps from term -> term_id\n",
    "    index = defaultdict(lambda: [])          # from term_id -> postings list\n",
    "\n",
    "    for doc_id, doc in block:\n",
    "        # append doc_id to the postings list of each term\n",
    "        for word in doc:\n",
    "            index[vocab[word]].append(doc_id)\n",
    "    \n",
    "    \n",
    "    # B. Sort terms\n",
    "    sorted_terms = sorted(vocab.keys())\n",
    "    print('\\n\\nBlock', block_id, [t + ' ' + str(index[vocab[t]]) for t in sorted_terms])\n",
    "    \n",
    "    # C. Write to disk\n",
    "    f = open('spimi_block' + str(block_id) + '.txt', 'wt')\n",
    "    f.write('\\n'.join(['%d, %s' % \n",
    "                      (vocab[t], str(index[vocab[t]])) \n",
    "                      for t in sorted_terms]))\n",
    "    f.close()\n",
    "    print\n",
    "    print('\\n\\t', vocab.items())\n",
    "    \n",
    "# Then, merge blocks in linear time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SPIMI\n",
    "\n",
    "Space requirements?\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Number of tokens in each block ($T$)\n",
    "- Number of unique terms in vocabulary *in each block* ($V_b << V$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Time requirements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sorting unique terms in each block \n",
    "- $O(V log V)$\n",
    "  - compare to $O(T log T)$ for BSBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce\n",
    "\n",
    "- What if we had 100K servers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **MapReduce:**\n",
    "  - A distributed programming framework\n",
    "  - Breaks large data into smaller data, called **splits**\n",
    "- Two phases:\n",
    "  - **Map:**\n",
    "    - Input: one split\n",
    "    - Output: (key, value) pairs\n",
    "  - **Reduce:**\n",
    "    - Input: (key, list of mapped values)\n",
    "    - Output: list of output values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce Counting Example\n",
    "\n",
    "```\n",
    "map(key, value):\n",
    "  for each word in value:\n",
    "    output word, 1\n",
    "       \n",
    "reduce(key, values):\n",
    "  output key, sum(values)\n",
    "```\n",
    "\n",
    "Framework takes care of grouping keys together to call `reduce` appropriately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce Counting Example\n",
    "\n",
    "- Split 1: \"Twinkle, twinkle little star\"\n",
    "- Split 2: \"Little by little\"\n",
    "\n",
    "#### Map\n",
    "\n",
    "\"Twinkle, twinkle little star\" $\\rightarrow$ **Mapper 1**  $\\rightarrow$ (twinkle, 1), (twinkle, 1), (little, 1), (star, 1)\n",
    "\n",
    "\"Little by little\" $\\rightarrow$ **Mapper 2** $\\rightarrow$ (little, 1), (by, 1), (little, 1)\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "(twinkle, 1), (twinkle, 1) $\\rightarrow$ **Reducer 1** $\\rightarrow$ (twinkle, 2)\n",
    "\n",
    "(little, 1), (little, 1), (little, 1) $\\rightarrow$ **Reducer 2** $\\rightarrow$ (little, 3)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Indexing with MapReduce\n",
    "\n",
    "- **Map**: Read a document and output (term, doc_id) pairs.\n",
    "- **Reduce**: Read a list of doc_ids for a term and output a postings list.\n",
    "\n",
    "\n",
    "\"Twinkle, twinkle little star\" $\\rightarrow$ **Mapper 1**  $\\rightarrow$ (twinkle, 0), (little, 0), (star, 0)\n",
    "\n",
    "\"Little by little\" $\\rightarrow$ **Mapper 2** $\\rightarrow$ (little, 1), (by, 1)\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "(little, 0), (little, 1) $\\rightarrow$ **Reducer 1** $\\rightarrow$ (little, [0, 1])\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from mrjob.job import MRJob  # install with `pip install mrjob`\n",
    "\n",
    "class MRIndexer(MRJob):\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        # Emit word, doc_id pairs from lines that look like:\n",
    "        # doc_id [document tokens]\n",
    "        words = re.findall('\\w+', line.lower())\n",
    "        doc_id = int(words[0])\n",
    "        for word in set(words[1:]):\n",
    "            yield word, doc_id\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # key is a term, values is an (unsorted) list of doc_ids\n",
    "        yield key, sorted(values)\n",
    "\n",
    "# Run python mr.py to execute this example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
